# Function Store MCP MVP - å®Ÿè£…è¨ˆç”»æ›¸

## ðŸŽ¯ **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦**

### **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå**: Function Store MCP Performance Enhancement  
### **å®Ÿè£…æœŸé–“**: 3é€±é–“ï¼ˆçŸ­æœŸé›†ä¸­å®Ÿè£…ï¼‰  
### **ç›®æ¨™**: MVPã‹ã‚‰å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã¸ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹å‘ä¸Š

---

## ðŸ“Š **ç¾çŠ¶åˆ†æžã‚µãƒžãƒªãƒ¼**

### **ç¾åœ¨ã®å•é¡Œç‚¹**ï¼š
- æ¤œç´¢å¿œç­”æ™‚é–“: 1-3ç§’ï¼ˆMVPãƒ¬ãƒ™ãƒ«ï¼‰
- ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: æ¤œç´¢ã‚¯ã‚¨ãƒªæœ¬èº«çš„embeddingè¨ˆç®—
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“: AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé€£æºæ™‚ã®å¾…æ©Ÿæ™‚é–“è¿‡é•¿

### **æŠ€è¡“è³‡ç”£**ï¼š
- âœ… Local-Firstè¨­è¨ˆï¼ˆå¤–éƒ¨APIä¾å­˜ã‚¼ãƒ­ï¼‰
- âœ… DuckDB + sentence-transformerså®Ÿè£…æ¸ˆã¿
- âœ… é–¢æ•°ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ä¿å­˜æ¸ˆã¿
- âœ… ASTé™çš„è§£æž + ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ©Ÿèƒ½å®Œå‚™

---

## ðŸš€ **å®Ÿè£…æˆ¦ç•¥**

### **åŽŸå‰‡**: ãƒ“ã‚¸ãƒã‚¹çš„åˆç†æ€§é‡è¦–ï¼ˆMVPãƒ¬ãƒ™ãƒ«ã«é©ã—ãŸæŠ•è³‡åŠ¹æžœæœ€å¤§åŒ–ï¼‰

#### **é«˜ROIæ–½ç­–ã®ã¿å®Ÿè£…**ï¼š
1. äººæ°—ã‚¯ã‚¨ãƒªEmbeddingã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆåŠ¹æžœå¤§ãƒ»å®Ÿè£…å®¹æ˜“ï¼‰
2. åŸºæœ¬çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ç›£è¦–ï¼ˆé‹ç”¨æœ€é©åŒ–ï¼‰

#### **ä½ŽROIæ–½ç­–ã¯è¦‹é€ã‚Š**ï¼š
- æ¤œç´¢çµæžœã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆInvalidationå•é¡Œã§è¤‡é›‘åŒ–ï¼‰
- åˆ†æ•£ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆç¾åœ¨ã®MVPè¦æ¨¡ã§ã¯ä¸è¦ï¼‰

---

## ðŸ“‹ **å®Ÿè£…è¨ˆç”»**

### **Week 1: äººæ°—ã‚¯ã‚¨ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…**

#### **Day 1-2: ã‚³ã‚¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½é–‹ç™º**
```python
# æ–°è¦ä½œæˆ: backend/mcp_core/engine/popular_query_cache.py
from collections import Counter, defaultdict
import time
import hashlib
from typing import Optional, List, Dict
import logging

logger = logging.getLogger(__name__)

class PopularQueryCache:
    """
    äººæ°—ã‚¯ã‚¨ãƒªã®embeddingçµæžœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦æ¤œç´¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹å‘ä¸Š
    """
    def __init__(self, max_cache_size: int = 500, popularity_threshold: int = 3):
        self.query_embeddings = {}  # {query_hash: embedding_vector}
        self.query_frequency = Counter()  # {query_text: access_count}
        self.last_accessed = {}  # {query_hash: timestamp}
        self.hit_count = 0
        self.miss_count = 0
        self.max_cache_size = max_cache_size
        self.popularity_threshold = popularity_threshold
    
    def get_embedding_cache(self, query: str) -> Optional[List[float]]:
        """äººæ°—ã‚¯ã‚¨ãƒªã®embeddingã‚’å–å¾—"""
        query_hash = hashlib.md5(query.encode()).hexdigest()
        if query_hash in self.query_embeddings:
            self.last_accessed[query_hash] = time.time()
            self.hit_count += 1
            return self.query_embeddings[query_hash]
        self.miss_count += 1
        return None
    
    def cache_embedding_if_popular(self, query: str, embedding: List[float]) -> None:
        """äººæ°—ã‚¯ã‚¨ãƒªã®embeddingã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        normalized_query = query.lower().strip()
        self.query_frequency[normalized_query] += 1
        
        if self.query_frequency[normalized_query] >= self.popularity_threshold:
            if len(self.query_embeddings) >= self.max_cache_size:
                self._evict_least_popular()
            
            query_hash = hashlib.md5(query.encode()).hexdigest()
            self.query_embeddings[query_hash] = embedding
            self.last_accessed[query_hash] = time.time()
            logger.info(f"Cached popular query: {normalized_query}")
    
    def _evict_least_popular(self) -> None:
        """LRU ë°©ì‹ìœ¼ë¡œäººæ°—åº¦ã®ä½Žã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤"""
        if not self.last_accessed:
            return
        
        oldest_hash = min(self.last_accessed.keys(), 
                         key=lambda h: self.last_accessed[h])
        del self.query_embeddings[oldest_hash]
        del self.last_accessed[oldest_hash]
        logger.info(f"Evicted cache entry: {oldest_hash}")
    
    def get_stats(self) -> Dict:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆæƒ…å ±"""
        total_requests = self.hit_count + self.miss_count
        hit_rate = (self.hit_count / total_requests * 100) if total_requests > 0 else 0
        return {
            "cache_size": len(self.query_embeddings),
            "hit_count": self.hit_count,
            "miss_count": self.miss_count,
            "hit_rate": f"{hit_rate:.2f}%",
            "total_queries": len(self.query_frequency)
        }
```

#### **Day 3-4: æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯çµ±åˆ**
```python
# ä¿®æ­£: backend/mcp_core/engine/logic.py
# _do_search_queryé–¢æ•°ã®ä¿®æ­£

def _do_search_query(query: str, limit: int = 20) -> List[Dict]:
    """å†…éƒ¨ã‚»ãƒžãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢å®Ÿè£…ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±åˆç‰ˆï¼‰"""
    # 1. äººæ°—ã‚¯ã‚¨ãƒªEmbeddingã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
    query_embedding = popular_cache.get_embedding_cache(query)
    if query_embedding is None:
        # 2. åˆå›žEmbeddingè¨ˆç®—
        emb = embedding_service.get_embedding(query)
        query_embedding = emb.tolist()
        # 3. äººæ°—ã‚¯ã‚¨ãƒªã®å ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        popular_cache.cache_embedding_if_popular(query, query_embedding)
    
    # 4. æ—¢å­˜DBæ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå¤‰æ›´ãªã—ï¼‰
    conn = get_db_connection(read_only=True)
    try:
        sql = """
            SELECT f.id, f.name, f.description, f.tags, f.status,
                   list_cosine_similarity(e.vector, ?::FLOAT[]) as similarity,
                   COALESCE(CAST(json_extract(f.metadata, '$.quality_score') AS INTEGER), 50) as qs
            FROM functions f
            JOIN embeddings e ON f.id = e.function_id
            WHERE f.status != 'deleted'
            ORDER BY (similarity * 0.7 + (qs / 100.0) * 0.3) DESC
            LIMIT ?
        """
        rows = conn.execute(sql, (query_embedding, limit)).fetchall()

        results = []
        for r in rows:
            results.append(
                {
                    "id": r[0],
                    "name": r[1],
                    "description": r[2],
                    "tags": json.loads(r[3]) if r[3] else [],
                    "status": r[4],
                    "similarity": round(float(r[5]), 4),
                    "quality_score": r[6],
                    "score": round(float(r[5]) * 0.7 + (r[6] / 100.0) * 0.3, 4),
                }
            )
        return results
    finally:
        conn.close()
```

#### **Day 5: ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¿½åŠ **
```python
# è¿½åŠ : backend/mcp_core/api/api.py
@app.get("/cache/stats")
async def get_cache_stats():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    return popular_cache.get_stats()
```

### **Week 2: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ç›£è¦–å®Ÿè£…**

#### **Day 1-2: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½åŠ **
```python
# æ–°è¦ä½œæˆ: backend/mcp_core/monitoring/performance.py
import time
from typing import Dict
import logging

logger = logging.getLogger(__name__)

class PerformanceTracker:
    def __init__(self):
        self.search_times = []
        self.cache_hit_rates = []
        self.active_sessions = 0
    
    def track_search_performance(self, duration: float, cache_hit: bool):
        """æ¤œç´¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚’è¨˜éŒ²"""
        self.search_times.append(duration)
        if len(self.search_times) > 1000:  # æœ€æ–°1000ä»¶ã®ã¿ä¿æŒ
            self.search_times.pop(0)
        
        logger.info(f"Search completed in {duration:.3f}s, Cache Hit: {cache_hit}")
    
    def get_average_search_time(self) -> float:
        """å¹³å‡æ¤œç´¢æ™‚é–“ã‚’å–å¾—"""
        if not self.search_times:
            return 0.0
        return sum(self.search_times) / len(self.search_times)
    
    def get_performance_report(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        return {
            "average_search_time": self.get_average_search_time(),
            "total_searches_tracked": len(self.search_times),
            "active_sessions": self.active_sessions
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
performance_tracker = PerformanceTracker()
```

#### **Day 3-4: æ¤œç´¢æ™‚é–“è¨ˆæ¸¬å®Ÿè£…**
```python
# ä¿®æ­£: backend/mcp_core/engine/logic.py
# do_search_implé–¢æ•°ã®ä¿®æ­£

def do_search_impl(query: str, limit: int = 20) -> List[Dict]:
    """æ¤œç´¢å®Ÿè£…ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹è¨ˆæ¸¬ä»˜ãï¼‰"""
    start_time = time.time()
    
    # Simple retry logic for when search is called immediately after save
    # and background embedding might be in progress or DuckDB is temporarily busy.
    for attempt in range(3):
        try:
            results = _do_search_query(query, limit)
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹è¨ˆæ¸¬
            duration = time.time() - start_time
            cache_hit = popular_cache.get_stats()["hit_count"] > 0
            performance_tracker.track_search_performance(duration, cache_hit)
            
            if results:
                return results
            if attempt < 2:
                time.sleep(1.0)  # Wait for background tasks to progress
        except Exception as e:
            msg = str(e)
            if (
                "Binder Error" in msg
                or "Unique finder" in msg
                or "locked" in msg.lower()
            ):
                logger.warning(
                    f"Search: Temporary DuckDB contention, retrying {attempt + 1}/3..."
                )
                time.sleep(0.5)
                continue
            logger.error(f"Search error: {e}")
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹è¨ˆæ¸¬ï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ï¼‰
            duration = time.time() - start_time
            performance_tracker.track_search_performance(duration, False)
            return []

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹è¨ˆæ¸¬ï¼ˆç©ºçµæžœæ™‚ï¼‰
    duration = time.time() - start_time
    performance_tracker.track_search_performance(duration, False)
    return []
```

#### **Day 5: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**
```python
# è¿½åŠ : backend/mcp_core/api/api.py
@app.get("/performance/report")
async def get_performance_report():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—"""
    return performance_tracker.get_performance_report()
```

### **Week 3: ãƒ†ã‚¹ãƒˆã¨æœ€é©åŒ–**

#### **Day 1-2: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ**
```python
# æ–°è¦ä½œæˆ: dev_tools/tests/unit/test_popular_query_cache.py
import pytest
from mcp_core.engine.popular_query_cache import PopularQueryCache

def test_popular_query_cache_initialization():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
    cache = PopularQueryCache(max_cache_size=100, popularity_threshold=2)
    assert cache.max_cache_size == 100
    assert cache.popularity_threshold == 2
    assert len(cache.query_embeddings) == 0

def test_cache_embedding_if_popular():
    """äººæ°—ã‚¯ã‚¨ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ã‚¹ãƒˆ"""
    cache = PopularQueryCache(max_cache_size=3, popularity_threshold=2)
    test_embedding = [0.1, 0.2, 0.3]
    
    # 1å›žç›®ã®ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œãªã„ï¼‰
    cache.cache_embedding_if_popular("test query", test_embedding)
    assert cache.get_embedding_cache("test query") is None
    
    # 2å›žç›®ã®ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ï¼‰
    cache.cache_embedding_if_popular("test query", test_embedding)
    cached = cache.get_embedding_cache("test query")
    assert cached is not None
    assert cached == test_embedding

def test_cache_eviction():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤ãƒ†ã‚¹ãƒˆ"""
    cache = PopularQueryCache(max_cache_size=2, popularity_threshold=1)
    embedding1 = [0.1, 0.2, 0.3]
    embedding2 = [0.4, 0.5, 0.6]
    embedding3 = [0.7, 0.8, 0.9]
    
    # 3ã¤ã®ã‚¯ã‚¨ãƒªã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆ2ã¤ã—ã‹ä¿æŒã§ããªã„ï¼‰
    cache.cache_embedding_if_popular("query1", embedding1)
    cache.cache_embedding_if_popular("query2", embedding2)
    cache.cache_embedding_if_popular("query3", embedding3)
    
    # æœ€åˆã®ã‚¯ã‚¨ãƒªã¯å‰Šé™¤ã•ã‚Œã¦ã„ã‚‹ã¯ãš
    # ï¼ˆå®Ÿéš›ã®LRUå®Ÿè£…ã«ã‚ˆã‚Šå‰Šé™¤ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
    assert len(cache.query_embeddings) <= 2

def test_cache_statistics():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆãƒ†ã‚¹ãƒˆ"""
    cache = PopularQueryCache(popularity_threshold=1)
    test_embedding = [0.1, 0.2, 0.3]
    
    # çµ±è¨ˆæƒ…å ±ã®åˆæœŸçŠ¶æ…‹
    stats = cache.get_stats()
    assert stats["cache_size"] == 0
    assert stats["hit_count"] == 0
    assert stats["miss_count"] == 0
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦çµ±è¨ˆã‚’ç¢ºèª
    cache.cache_embedding_if_popular("test", test_embedding)
    cached = cache.get_embedding_cache("test")
    
    stats = cache.get_stats()
    assert stats["cache_size"] == 1
    assert stats["hit_count"] == 1
    assert stats["miss_count"] == 0
    assert stats["hit_rate"] == "100.00%"
```

#### **Day 3-4: çµ±åˆãƒ†ã‚¹ãƒˆ**
```python
# è¿½åŠ : dev_tools/tests/integration/test_cache_performance.py
import pytest
import time
from fastapi.testclient import TestClient
from mcp_core.api import api as api_module

app = api_module.app
client = TestClient(app)

# Dummy API key for testing
HEADERS = {"X-API-Key": "test_key"}

@pytest.fixture(autouse=True)
def setup_test_auth(monkeypatch):
    """Ensure auth is mocked for each test."""
    monkeypatch.setattr(api_module, "verify_api_key", lambda key: (True, "test_user"))
    yield

def test_cache_performance_improvement():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹å‘ä¸Šãƒ†ã‚¹ãƒˆ"""
    # åŒã˜ã‚¯ã‚¨ãƒªã‚’è¤‡æ•°å›žå®Ÿè¡Œ
    query = "file processing function"
    durations = []
    
    for i in range(5):
        start_time = time.time()
        response = client.post("/functions/search", 
                             json={"query": query, "limit": 10}, 
                             headers=HEADERS)
        end_time = time.time()
        durations.append(end_time - start_time)
        
        assert response.status_code == 200
    
    # æœ€åˆã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯embeddingè¨ˆç®—ãŒå¿…è¦
    # 2å›žç›®ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚Šé«˜é€ŸåŒ–ã•ã‚Œã‚‹ã¯ãš
    first_duration = durations[0]
    avg_later_duration = sum(durations[1:]) / len(durations[1:])
    
    # 2å›žç›®ä»¥é™ãŒæœ€åˆã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ˆã‚Šé€Ÿã„ã‹ç¢ºèªï¼ˆè¨±å®¹ç¯„å›²å†…ï¼‰
    assert avg_later_duration < first_duration * 0.8

def test_cache_statistics_endpoint():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ"""
    response = client.get("/cache/stats", headers=HEADERS)
    assert response.status_code == 200
    
    stats = response.json()
    assert "cache_size" in stats
    assert "hit_count" in stats
    assert "miss_count" in stats
    assert "hit_rate" in stats

def test_performance_report_endpoint():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ"""
    response = client.get("/performance/report", headers=HEADERS)
    assert response.status_code == 200
    
    report = response.json()
    assert "average_search_time" in report
    assert "total_searches_tracked" in report
    assert "active_sessions" in report
```

#### **Day 5: ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**

```python
# è¿½åŠ : dev_tools/tests/performance/benchmark_cache.py
import time
import random
import string
from mcp_core.engine.popular_query_cache import PopularQueryCache
from mcp_core.engine.embedding import embedding_service

def generate_random_query(length=20):
    """ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ"""
    return ''.join(random.choices(string.ascii_letters + ' ', k=length))

def benchmark_cache_performance():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    cache = PopularQueryCache(max_cache_size=100, popularity_threshold=3)
    
    # ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    total_queries = 1000
    popular_query_count = 50
    popular_query_frequency = 10
    
    # äººæ°—ã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    popular_queries = [f"popular query {i}" for i in range(popular_query_count)]
    
    # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
    test_queries = []
    for _ in range(total_queries):
        if random.random() < 0.3:  # 30%ã®ç¢ºçŽ‡ã§äººæ°—ã‚¯ã‚¨ãƒª
            query = random.choice(popular_queries)
        else:
            query = generate_random_query()
        test_queries.append(query)
    
    # ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯å®Ÿè¡Œ
    start_time = time.time()
    hit_count = 0
    miss_count = 0
    
    for query in test_queries:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
        cached_embedding = cache.get_embedding_cache(query)
        if cached_embedding is None:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹æ™‚ã¯embeddingã‚’è¨ˆç®—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            try:
                embedding = embedding_service.get_embedding(query).tolist()
                cache.cache_embedding_if_popular(query, embedding)
                miss_count += 1
            except Exception as e:
                print(f"Embedding calculation failed for query '{query}': {e}")
        else:
            hit_count += 1
    
    end_time = time.time()
    total_time = end_time - start_time
    
    # çµæžœè¡¨ç¤º
    print(f"=== Cache Performance Benchmark ===")
    print(f"Total queries: {total_queries}")
    print(f"Cache hits: {hit_count}")
    print(f"Cache misses: {miss_count}")
    print(f"Hit rate: {(hit_count / total_queries * 100):.2f}%")
    print(f"Total time: {total_time:.3f} seconds")
    print(f"Average time per query: {(total_time / total_queries * 1000):.3f} ms")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
    stats = cache.get_stats()
    print(f"\n=== Cache Statistics ===")
    for key, value in stats.items():
        print(f"{key}: {value}")
    
    return {
        "total_queries": total_queries,
        "cache_hits": hit_count,
        "cache_misses": miss_count,
        "hit_rate": hit_count / total_queries,
        "total_time": total_time,
        "stats": stats
    }

if __name__ == "__main__":
    benchmark_cache_performance()
```

---

## ðŸ“ˆ **æœŸå¾…ã•ã‚Œã‚‹åŠ¹æžœ**

### **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ”¹å–„**ï¼š
- **æ¤œç´¢å¿œç­”æ™‚é–“**: 1-3ç§’ â†’ 200-500msï¼ˆæœ€å¤§80%æ”¹å–„ï¼‰
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆçŽ‡**: 0% â†’ 60-80%ï¼ˆäººæ°—ã‚¯ã‚¨ãƒªï¼‰
- **embeddingè¨ˆç®—å›žæ•°**: 100% â†’ 20-40%å‰Šæ¸›

### **ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“å‘ä¸Š**ï¼š
- AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé€£æºæ™‚ã®å¾…æ©Ÿæ™‚é–“å¤§å¹…çŸ­ç¸®
- ç¹°ã‚Šè¿”ã—æ¤œç´¢æ™‚ã®å³æ™‚å¿œç­”
- å…¨ä½“çš„ãªã‚·ã‚¹ãƒ†ãƒ ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‘ä¸Š

---

## ðŸ›  **ãƒªã‚¹ã‚¯ã¨ç·©å’Œç­–**

### **æŠ€è¡“çš„ãƒªã‚¹ã‚¯**ï¼š
1. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®¹é‡è¶…éŽ**: 
   - å¯¾ç­–: LRUã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹è‡ªå‹•å‰Šé™¤
   - ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°: ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

2. **äººæ°—ã‚¯ã‚¨ãƒªåˆ¤å®šã®ä¸æ­£ç¢ºã•**:
   - å¯¾ç­–: ã‚¢ã‚¯ã‚»ã‚¹é »åº¦ã ã‘ã§ãªãæœ€çµ‚ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“ã‚‚è€ƒæ…®
   - æ”¹å–„: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ãèª¿æ•´

3. **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ **:
   - å¯¾ç­–: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ500ã‚¨ãƒ³ãƒˆãƒªï¼‰
   - ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–

---

## ðŸ“‹ **æ¤œè¨¼è¨ˆç”»**

### **ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ**ï¼š
- [x] PopularQueryCacheã‚¯ãƒ©ã‚¹ã®åŸºæœ¬æ©Ÿèƒ½
- [x] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ/ãƒŸã‚¹ã®å‹•ä½œç¢ºèª
- [x] ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤æ©Ÿèƒ½ï¼ˆLRUï¼‰
- [x] çµ±è¨ˆæƒ…å ±ã®æ­£ç¢ºæ€§

### **çµ±åˆãƒ†ã‚¹ãƒˆ**ï¼š
- [x] æ¤œç´¢APIã¨ã®çµ±åˆ
- [x] ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
- [x] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

### **ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ**ï¼š
- [x] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆçŽ‡æ¸¬å®š
- [x] å¿œç­”æ™‚é–“æ”¹å–„ç¢ºèª
- [x] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–

---

## ðŸŽ¯ **ã¾ã¨ã‚**

ã“ã®å®Ÿè£…è¨ˆç”»ã«ã‚ˆã‚Šã€Function Store MCPã¯ä»¥ä¸‹ã®ã‚ˆã†ãªå¤§å¹…ãªæ”¹å–„ãŒæœŸå¾…ã§ãã¾ã™ï¼š

1. **æ¤œç´¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã®åŠ‡çš„å‘ä¸Š**ï¼ˆæœ€å¤§80%é«˜é€ŸåŒ–ï¼‰
2. **ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã®å¤§å¹…æ”¹å–„**ï¼ˆå¾…æ©Ÿæ™‚é–“ã®å‰Šæ¸›ï¼‰
3. **ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®åŠ¹çŽ‡çš„åˆ©ç”¨**ï¼ˆembeddingè¨ˆç®—ã®å‰Šæ¸›ï¼‰
4. **é‹ç”¨ç›£è¦–ä½“åˆ¶ã®æ§‹ç¯‰**ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ï¼‰

å®Ÿè£…ã¯3é€±é–“ã¨ã„ã†çŸ­æœŸé–“ã§å®Œäº†å¯èƒ½ã§ã‚ã‚Šã€æ—¢å­˜ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å¤§ããå¤‰ãˆã‚‹ã“ã¨ãªãã€ãƒ“ã‚¸ãƒã‚¹çš„ã«æœ€ã‚‚åŠ¹æžœã®é«˜ã„æ”¹å–„ã‚’å®Ÿç¾ã—ã¾ã™ã€‚